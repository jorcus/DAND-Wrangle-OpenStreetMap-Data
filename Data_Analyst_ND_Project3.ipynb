{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered in the Map\n",
    "Discuss the main problems with the data in the following order:\n",
    "\n",
    "- Over­abbreviated street names (“S Tryon St Ste 105”)\n",
    "\n",
    "- Second level “k” tags with the value \"type\"(which overwrites the element’s previously processed node[“type”]field).\n",
    "\n",
    "- Street names in second ­level “k” tags pulled from Tiger GPS data and divided into segments, in the following format:\n",
    "\n",
    "- Unstructure Unique ID (1, 42653, 2321, 5030230)\n",
    "\n",
    "\n",
    "###  Map Area - Dataset\n",
    "\n",
    "In this project, I choose San Jose which is a large city surrounded by rolling hills in Silicon Valley, a major technology hub in California's Bay Area. I want to learn more about the place to see what database querying reveals. This location is one of my dreams working area as it's all over the world-class Tech corporations around there. \n",
    "\n",
    "San Jose, United States (OSM XML: 364.6 MB)\n",
    "- https://mapzen.com/data/metro-extracts/metro/san-jose_california/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset folder: ./san-jose_california.osm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pprint\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "\n",
    "DATASET = \"san-jose_california.osm\" # osm filename\n",
    "PATH = \"./\" # directory contain the osm file\n",
    "OSMFILE = PATH + DATASET\n",
    "print('Dataset folder:', OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Parsing the OSM file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of tag:  8\n",
      "Numbers of tag elements:  4599618\n",
      "{'bounds': 1,\n",
      " 'member': 18333,\n",
      " 'nd': 1965111,\n",
      " 'node': 1679378,\n",
      " 'osm': 1,\n",
      " 'relation': 1759,\n",
      " 'tag': 705634,\n",
      " 'way': 229401}\n"
     ]
    }
   ],
   "source": [
    "# mapparser.py\n",
    "# iterative parsing\n",
    "from mapparser import count_tags, count_tags_total\n",
    "\n",
    "tags = count_tags(OSMFILE)\n",
    "print('Numbers of tag: ', len(tags))\n",
    "print('Numbers of tag elements: ', count_tags_total(tags))\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize the tag keys.\n",
    "Categorize the tag keys in the followings:\n",
    "- \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "- \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "- \"problemchars\", for tags with problematic characters, and\n",
    "- \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 459030, 'lower_colon': 224633, 'other': 21969, 'problemchars': 2}\n"
     ]
    }
   ],
   "source": [
    "# tags.py\n",
    "from tags import key_type\n",
    "def process_map_tags(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "keys = process_map_tags(OSMFILE)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Users\n",
    "As you can see, each of the user has their own unique ID. However, the ID is unstructured likes 1, 1005885, 1030, 100744. I structured all the unique user id in the followings:\n",
    "- 25663 => 0025663\n",
    "- 951370 => 0951370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  1359\n",
      "User ID maximum length 7\n",
      "25663 => 0025663\n",
      "951370 => 0951370\n",
      "199089 => 0199089\n",
      "637707 => 0637707\n",
      "28145 => 0028145\n",
      "941449 => 0941449\n",
      "281267 => 0281267\n",
      "41907 => 0041907\n",
      "166129 => 0166129\n",
      "173623 => 0173623\n"
     ]
    }
   ],
   "source": [
    "# users.py\n",
    "from users import unique_user_id, max_length_user_id, structure_user_id\n",
    "\n",
    "def test():\n",
    "    users = unique_user_id(OSMFILE)\n",
    "    # structured = structure_user_id(users)\n",
    "    # pprint.pprint(structured)\n",
    "    max_length = max_length_user_id(users)\n",
    "    print('Number of users: ', len(users))\n",
    "    print('User ID maximum length', max_length)\n",
    "\n",
    "    print_limit = 10\n",
    "    for user_id in users:\n",
    "        if len(user_id) < max_length:\n",
    "            structured_id = user_id\n",
    "            while len(structured_id) < max_length:\n",
    "                structured_id = str('0' + structured_id)\n",
    "\n",
    "            if print_limit > 0:\n",
    "                print_limit -= 1\n",
    "                print(user_id, \"=>\", structured_id)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-abbreviated Street Names\n",
    "Some basic query is over-abbreviated. I updated all the problematic address strings in the followings:\n",
    "\n",
    "- Seaboard Ave => Seaboard Avenue\n",
    "- Cherry Ave => Cherry Avenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hillsdale Ave => Hillsdale Avenue\n",
      "Meridian Ave => Meridian Avenue\n",
      "Walsh Ave => Walsh Avenue\n",
      "Seaboard Ave => Seaboard Avenue\n",
      "N Blaney Ave => N Blaney Avenue\n",
      "Saratoga Ave => Saratoga Avenue\n",
      "1425 E Dunne Ave => 1425 E Dunne Avenue\n",
      "Blake Ave => Blake Avenue\n",
      "The Alameda Ave => The Alameda Avenue\n",
      "Hollenbeck Ave => Hollenbeck Avenue\n"
     ]
    }
   ],
   "source": [
    "#audit.py\n",
    "from audit import audit, update_name, street_type_re, mapping\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "    # pprint.pprint(dict(st_types)) #print out dictonary of potentially incorrect street types\n",
    "    print_limit = 10\n",
    "    for st_type, ways in st_types.items(): # .iteritems() for python2\n",
    "        for name in ways:\n",
    "            if street_type_re.search(name).group() in mapping:\n",
    "                better_name = update_name(name, mapping)\n",
    "                if print_limit > 0:\n",
    "                    print_limit -= 1\n",
    "                    print (name, \"=>\", better_name)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data into Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "from data import process_map\n",
    "\n",
    "data = process_map(OSMFILE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created': {'changeset': '11686320',\n",
       "  'timestamp': '2012-05-24T03:24:59Z',\n",
       "  'uid': '0014293',\n",
       "  'user': 'KindredCoda',\n",
       "  'version': '10'},\n",
       " 'id': '25457954',\n",
       " 'pos': [37.1582245, -121.6574737],\n",
       " 'type': 'node',\n",
       " 'visible': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.SanJose\n",
    "collection = db.SanJoseMAP\n",
    "#collection.insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'SanJose'), 'SanJoseMAP')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the original xml file:  348.08773612976074 MB\n",
      "Size of the processed json file:  512.8097190856934 MB\n",
      "Number of documents: 19761132\n",
      "Number of nodes: 17470242\n",
      "Number of ways: 2290674\n",
      "Number of relations: 0\n",
      "Number of unique users: 1356\n",
      "Number of pizza places: 636\n"
     ]
    }
   ],
   "source": [
    "print('Size of the original xml file: ',os.path.getsize(OSMFILE)/(1024*1024.0), 'MB')\n",
    "print('Size of the processed json file: ',os.path.getsize(os.path.join(PATH, \"san-jose_california.osm.json\"))/(1024*1024.0), 'MB')\n",
    "print('Number of documents: ' + str(collection.find().count()))\n",
    "print('Number of nodes: ' + str(collection.find({\"type\":\"node\"}).count()))\n",
    "print('Number of ways: ' + str(collection.find({\"type\":\"way\"}).count()))\n",
    "print('Number of relations: ' + str(collection.find({\"type\":\"relation\"}).count()))\n",
    "print('Number of unique users: ' + str(len(collection.distinct(\"created.user\"))))\n",
    "print('Number of pizza places: ' + str(collection.find({\"cuisine\":\"pizza\"}).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributor statistics and gamification suggestio\n",
    "\n",
    "The contributions of users seems incredibly skewed, possibly due to automated versus manual map editing (the word “bot” appears in some usernames). Here are some user percentage statistics:\n",
    "- Top user contribution percentage (“nmixter”) - 15.08%\n",
    "- Combined top 2 users' contribution (“nmixter” and “andygol”) - 30.07%\n",
    "- Combined Top 10 users contribution - 64.12%\n",
    "\n",
    "Thinking about these user percentages, I’m reminded of “gamification” as a motivating force for contribution. In the context of the OpenStreetMap, if user data were more prominently displayed, perhaps others would take an initiative in submitting more edits to the map. It's so surprise, the only top 10 users that contributed over than 50% of this dataset. That might spur the creation of more efficient bots, especially if certain gamification elements were present, such as rewards, badges, or a leaderboard.           \n",
    "\n",
    "#### Top 10 users with most contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'nmixter', 'count': 2980568}\n",
      "{'_id': 'andygol', 'count': 2961664}\n",
      "{'_id': 'mk408', 'count': 1615791}\n",
      "{'_id': 'Bike Mapper', 'count': 969105}\n",
      "{'_id': 'samely', 'count': 813227}\n",
      "{'_id': 'RichRico', 'count': 768741}\n",
      "{'_id': 'dannykath', 'count': 752101}\n",
      "{'_id': 'MustangBuyer', 'count': 646129}\n",
      "{'_id': 'karitotp', 'count': 645535}\n",
      "{'_id': 'Minh Nguyen', 'count': 517383}\n"
     ]
    }
   ],
   "source": [
    "# Top 10 users with most contributions\n",
    "pipeline = [{\"$group\":{\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\": 10}]\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(10):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of users appearing only once (having 1 post)\n",
    "##### There's only one user appearing only once. Which means most the the user appear at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 1, 'num_users': 1}\n"
     ]
    }
   ],
   "source": [
    "# Number of users appearing only once (having 1 post)\n",
    "pipeline = [{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}},\n",
    "                      {\"$group\":{\"_id\":\"$count\", \"num_users\":{\"$sum\":1}}},\n",
    "                      {\"$sort\":{\"_id\":1}}, {\"$limit\":1}]\n",
    "\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(1):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Biggest religion\n",
    "##### The result show in Sanjose area, the Christian is one of the biggest religion. The the seconds largest is Unknown, seem like the record is missing. Then coming to the third largest religion is jewish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'christian', 'count': 1996}\n",
      "{'_id': None, 'count': 139}\n",
      "{'_id': 'jewish', 'count': 33}\n",
      "{'_id': 'buddhist', 'count': 26}\n",
      "{'_id': 'muslim', 'count': 18}\n",
      "{'_id': 'hindu', 'count': 14}\n",
      "{'_id': 'unitarian_universalist', 'count': 13}\n",
      "{'_id': 'sikh', 'count': 7}\n",
      "{'_id': 'caodaism', 'count': 7}\n",
      "{'_id': 'zoroastrian', 'count': 7}\n"
     ]
    }
   ],
   "source": [
    "# Top 10 Biggest religion\n",
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"place_of_worship\"}},\n",
    "            {\"$group\":{\"_id\":\"$religion\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}}, {\"$limit\":10}]\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(10):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 appearing amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '', 'count': 3798126}\n",
      "{'_id': 'parking', 'count': 12302}\n",
      "{'_id': 'restaurant', 'count': 6573}\n",
      "{'_id': 'fast_food', 'count': 3406}\n",
      "{'_id': 'school', 'count': 3321}\n",
      "{'_id': 'place_of_worship', 'count': 2298}\n",
      "{'_id': 'bench', 'count': 1807}\n",
      "{'_id': 'cafe', 'count': 1753}\n",
      "{'_id': 'fuel', 'count': 1580}\n",
      "{'_id': 'bicycle_parking', 'count': 1347}\n"
     ]
    }
   ],
   "source": [
    "# Top 10 appearing amenities\n",
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}}, {\"$limit\":10}]\n",
    "\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(10):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': None, 'count': 1296}\n",
      "{'_id': '', 'count': 572}\n",
      "{'_id': 'mexican', 'count': 570}\n",
      "{'_id': 'chinese', 'count': 504}\n",
      "{'_id': 'vietnamese', 'count': 459}\n",
      "{'_id': 'pizza', 'count': 390}\n",
      "{'_id': 'japanese', 'count': 293}\n",
      "{'_id': 'american', 'count': 283}\n",
      "{'_id': 'italian', 'count': 222}\n",
      "{'_id': 'indian', 'count': 214}\n"
     ]
    }
   ],
   "source": [
    "# Top 10 popular cuisines\n",
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"restaurant\"}},\n",
    "            {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}}, {\"$limit\":10}]\n",
    "\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(10):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort postcodes by count, descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '', 'count': 1893917}\n",
      "{'_id': '95014', 'count': 3503}\n",
      "{'_id': '95070', 'count': 2438}\n",
      "{'_id': '94087', 'count': 2205}\n",
      "{'_id': '94086', 'count': 2052}\n",
      "{'_id': '95051', 'count': 1772}\n",
      "{'_id': '95129', 'count': 1397}\n",
      "{'_id': '95127', 'count': 1130}\n",
      "{'_id': '95054', 'count': 1023}\n",
      "{'_id': '95035', 'count': 1018}\n"
     ]
    }
   ],
   "source": [
    "# Sort postcodes by count, descending\n",
    "pipeline = [{\"$match\":{\"address.postcode\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$address.postcode\", \"count\":{\"$sum\":1}}}, \n",
    "            {\"$sort\":{\"count\":-1}}]\n",
    "\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(10):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort street by count, descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '', 'count': 1885122}\n",
      "{'_id': 'Stevens Creek Boulevard', 'count': 2898}\n",
      "{'_id': 'Hollenbeck Avenue', 'count': 1745}\n",
      "{'_id': 'South Stelling Road', 'count': 1300}\n",
      "{'_id': 'East Estates Drive', 'count': 1230}\n",
      "{'_id': 'Johnson Avenue', 'count': 1200}\n",
      "{'_id': 'Miller Avenue', 'count': 1170}\n",
      "{'_id': 'Bollinger Road', 'count': 1170}\n",
      "{'_id': 'North Santa Cruz Avenue', 'count': 1160}\n",
      "{'_id': 'South De Anza Boulevard', 'count': 1127}\n"
     ]
    }
   ],
   "source": [
    "# Sort street by count, descending\n",
    "pipeline = [{\"$match\":{\"address.street\":{\"$exists\":1}}},\n",
    "            {\"$group\":{\"_id\":\"$address.street\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}}]\n",
    "\n",
    "result = collection.aggregate(pipeline)\n",
    "for r in range(10):\n",
    "    print (result.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas            \n",
    "\n",
    "\n",
    "## Conclusion\n",
    "I believe it has been cleaned for the purposes of this exercise. However, some area of the San Jose data is obviously far from being complete. There's still some data haven't clean likes Inconsistent postal codes (“NC28226”, “28226­0783”, “28226”) and “Incorrect” postal codes (Charlotte area zip codes all begin with “282” however a large portion of all documented zip codes were outside this region.)\n",
    "\n",
    "## References\n",
    "\n",
    "[1] OpenStreetMap Sample Project Data Wrangling with MongoDB (Matthew Banbury). Available from: <https://docs.google.com/document/d/1F0Vs14oNEs2idFJR3C_OPxwS6L0HPliOii-QpbmrMo4/pub>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
